We hit a nasty CUDA error that meant we can no longer run nvblox and VSLAM concurrently.

## Bisection
Rolling back commits to find the commit where this occurred.
```
rolling back nvblox_lib from 0.0.10 -> 0.0.9           NO FIX
rolling back GXF to pre-23.03 			               NO FIX		
rolling back nvblox_lib from 0.0.9 -> 0.0.7		       FIX 		       e3a848a77aed
rolling forward to one commit after 		           WORKS 		   15390933598c2d0025e9420bef54ddaeabd80a1a
rolling forward to just after cuvslam upgrade 	       BREAKS          6a52024960f442e5bee20578bc123e4038c2e9f6
rolling back 2 commits to just before		           WORKS 		   aed4b094cd32069ffba5aeea6c091f2b74770812
rolling forward 1 onto *exactly* cuVSLAM upgrade       BREAKS          5528bb50b8edbc95836194ce335594964f439682
```
We have a winner:
`[ext/vslam] switch to cuVSLAM 11 binaries`
with hash:
`5528bb50b8edbc95836194ce335594964f439682`


## Things we did
* Run GDB from the sandbox to see that different functions were triggering
* Compile everything in debug
* Remove all eigen alignement
* Run memcheck from the sandbox
* Run compute-sanitizer from the sandbox
	* Required changing BlockMemoryPool allocator to UnboundedAllocator
	* **This was the only thing that seemed to provide clues.**

To run the compute sanitizer:
```bash
compute-sanitizer ./external/com_nvidia_gxf/gxf/gxe/gxe --app extensions/nvblox/apps/realsense/nvblox_realsense.yaml --manifest extensions/nvblox/apps/realsense/nvblox_realsense_manifest.yaml 2&>1 > error.txt
```

## Notes
David was good with GDB. You can set breakpoints on the command line.
